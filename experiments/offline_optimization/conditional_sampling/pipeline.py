from __future__ import annotations

import csv
import json
import logging
import math
from dataclasses import dataclass, field
from pathlib import Path  # noqa: TC003
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from collections.abc import Sequence

import numpy as np
import torch

from joint_improvement.hyformer import BackboneBundle

_LOGGER = logging.getLogger(__name__)


@dataclass
class ConditionalSamplingConfig:
    """Configuration for the conditional sampling stage."""

    seed: int
    sequences: Sequence[str]
    scores: Sequence[float] | None
    backbone: BackboneBundle | None
    output_csv: Path
    output_json: Path
    prompt_template: str
    num_prompts: int
    samples_per_prompt: int
    property_threshold: float | None = None
    top_k: int | None = None
    max_new_tokens: int = 64
    temperature: float = 0.8
    top_p: float = 0.95
    repetition_penalty: float = 1.0
    device: str = "cpu"
    dry_run: bool = False
    model_id: str | None = None
    score_lookup: dict[str, float] | None = None
    extra_metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class ConditionalSamplingResult:
    """Outputs generated by the conditional sampling stage."""

    rows: list[dict[str, Any]]
    csv_path: Path
    json_path: Path


def run_conditional_sampling(
    config: ConditionalSamplingConfig,
) -> ConditionalSamplingResult:
    """Generate candidate molecules conditioned on high-value sequences."""
    _LOGGER.info("Running conditional sampling | seed=%s", config.seed)
    if not config.sequences:
        raise ValueError("No sequences available for conditional sampling.")

    device = torch.device(config.device)
    score_array = _normalise_scores(config.scores)
    candidate_indices = _select_candidates(config, score_array)
    prompts = _build_prompts(config, candidate_indices, score_array)

    rows = _generate_candidates(config, prompts, device)
    _write_rows_to_csv(config.output_csv, rows)
    summary = _summarise_rows(config, rows)

    config.output_json.parent.mkdir(parents=True, exist_ok=True)
    with config.output_json.open("w", encoding="utf-8") as handle:
        json.dump(summary, handle, indent=2)

    _LOGGER.info(
        "Conditional sampling finished | seed=%s candidates=%s unique=%s",
        config.seed,
        len(rows),
        summary["unique_candidates"],
    )
    return ConditionalSamplingResult(
        rows=rows,
        csv_path=config.output_csv,
        json_path=config.output_json,
    )


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _normalise_scores(scores: Sequence[float] | None) -> np.ndarray | None:
    if scores is None:
        return None
    array = np.asarray(scores, dtype=np.float32)
    if array.ndim != 1:
        array = array.reshape(-1)
    return array


def _select_candidates(
    config: ConditionalSamplingConfig,
    score_array: np.ndarray | None,
) -> np.ndarray:
    indices = np.arange(len(config.sequences))

    if config.property_threshold is not None or config.top_k is not None:
        if score_array is None:
            raise ValueError("property_threshold/top_k require dataset scores.")

    if config.property_threshold is not None:
        indices = indices[score_array >= config.property_threshold]

    if config.top_k is not None:
        order = np.argsort(score_array)[::-1]
        indices = order[: config.top_k]

    if len(indices) == 0:
        raise ValueError("Candidate filtering removed all sequences.")

    if len(indices) < config.num_prompts:
        repeats = math.ceil(config.num_prompts / len(indices))
        indices = np.tile(indices, repeats)

    rng = np.random.default_rng(config.seed)
    rng.shuffle(indices)
    return indices[: config.num_prompts]


def _build_prompts(
    config: ConditionalSamplingConfig,
    candidate_indices: np.ndarray,
    score_array: np.ndarray | None,
) -> list[dict[str, Any]]:
    prompts: list[dict[str, Any]] = []
    for prompt_idx, data_idx in enumerate(candidate_indices):
        conditioning_sequence = config.sequences[int(data_idx)]
        conditioning_score = None
        if score_array is not None:
            conditioning_score = float(score_array[int(data_idx)])
        formatted_score = "unknown"
        if conditioning_score is not None:
            formatted_score = f"{conditioning_score:.4f}"
        prompt_text = config.prompt_template.format(
            sequence=conditioning_sequence,
            score=formatted_score,
            rank=prompt_idx,
        )
        prompts.append(
            {
                "prompt_index": prompt_idx,
                "conditioning_sequence": conditioning_sequence,
                "conditioning_score": conditioning_score,
                "prompt_text": prompt_text,
            }
        )
    return prompts


def _generate_candidates(
    config: ConditionalSamplingConfig,
    prompts: list[dict[str, Any]],
    device: torch.device,
) -> list[dict[str, Any]]:
    if not prompts:
        return []

    if config.dry_run:
        return _generate_dry_run(config, prompts)

    if config.backbone is None:
        raise ValueError("Backbone must be provided unless dry_run is enabled.")

    tokenizer = config.backbone.tokenizer
    if tokenizer is None:
        raise ValueError(
            "The selected backbone does not expose a tokenizer required for sampling.",
        )
    model = config.backbone.model
    model.eval()
    model.to(device)

    pad_token_id = tokenizer.pad_token_id
    if pad_token_id is None:
        eos_token_id = tokenizer.eos_token_id
        if eos_token_id is None:
            raise ValueError("Tokenizer must define pad_token_id or eos_token_id.")
        pad_token_id = eos_token_id
        try:
            tokenizer.pad_token_id = pad_token_id
        except AttributeError:
            pass
        pad_token_missing = getattr(tokenizer, "pad_token", None) is None
        if pad_token_missing and tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token

    prompt_texts = [prompt["prompt_text"] for prompt in prompts]
    inputs = tokenizer(
        prompt_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
    ).to(device)

    torch.manual_seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)

    outputs = model.generate(
        **inputs,
        max_new_tokens=config.max_new_tokens,
        temperature=config.temperature,
        top_p=config.top_p,
        repetition_penalty=config.repetition_penalty,
        do_sample=True,
        num_return_sequences=config.samples_per_prompt,
        pad_token_id=pad_token_id,
    )

    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    rows: list[dict[str, Any]] = []
    for prompt_idx, prompt in enumerate(prompts):
        for sample_id in range(config.samples_per_prompt):
            output_index = prompt_idx * config.samples_per_prompt + sample_id
            raw_text = decoded[output_index]
            generated = _strip_prompt(prompt["prompt_text"], raw_text)
            score = None
            if config.score_lookup:
                score = config.score_lookup.get(generated)
                if score is not None:
                    score = float(score)

            rows.append(
                {
                    "seed": config.seed,
                    "model_id": config.model_id,
                    "prompt_index": prompt["prompt_index"],
                    "sample_index": sample_id,
                    "conditioning_sequence": prompt["conditioning_sequence"],
                    "conditioning_score": prompt["conditioning_score"],
                    "prompt": prompt["prompt_text"],
                    "generated_sequence": generated,
                    "score": score,
                }
            )
    return rows


def _generate_dry_run(
    config: ConditionalSamplingConfig,
    prompts: list[dict[str, Any]],
) -> list[dict[str, Any]]:
    rows: list[dict[str, Any]] = []
    for prompt in prompts:
        for sample_id in range(config.samples_per_prompt):
            generated = f"{prompt['conditioning_sequence']}_seed{config.seed}_sample{sample_id}"
            score = None
            if config.score_lookup:
                score = config.score_lookup.get(generated)
                if score is not None:
                    score = float(score)

            rows.append(
                {
                    "seed": config.seed,
                    "model_id": config.model_id,
                    "prompt_index": prompt["prompt_index"],
                    "sample_index": sample_id,
                    "conditioning_sequence": prompt["conditioning_sequence"],
                    "conditioning_score": prompt["conditioning_score"],
                    "prompt": prompt["prompt_text"],
                    "generated_sequence": generated,
                    "score": score,
                }
            )
    return rows


def _strip_prompt(prompt: str, generated: str) -> str:
    if generated.startswith(prompt):
        return generated[len(prompt) :].strip()
    return generated.strip()


def _write_rows_to_csv(path: Path, rows: list[dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        "seed",
        "model_id",
        "prompt_index",
        "sample_index",
        "conditioning_sequence",
        "conditioning_score",
        "prompt",
        "generated_sequence",
        "score",
    ]
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            csv_row = row.copy()
            if csv_row["conditioning_score"] is None:
                csv_row["conditioning_score"] = ""
            if csv_row["score"] is None:
                csv_row["score"] = ""
            writer.writerow(csv_row)


def _summarise_rows(
    config: ConditionalSamplingConfig,
    rows: list[dict[str, Any]],
) -> dict[str, Any]:
    scores = [row["score"] for row in rows if isinstance(row["score"], (int, float))]
    unique_generated = len({row["generated_sequence"] for row in rows})
    summary: dict[str, Any] = {
        "seed": config.seed,
        "model_id": config.model_id,
        "num_prompts": config.num_prompts,
        "samples_per_prompt": config.samples_per_prompt,
        "num_candidates": len(rows),
        "unique_candidates": unique_generated,
        "scored_candidates": len(scores),
        "mean_score": float(np.mean(scores)) if scores else None,
        "max_score": float(np.max(scores)) if scores else None,
        "config": {
            "property_threshold": config.property_threshold,
            "top_k": config.top_k,
            "max_new_tokens": config.max_new_tokens,
            "temperature": config.temperature,
            "top_p": config.top_p,
            "repetition_penalty": config.repetition_penalty,
            "device": config.device,
            "dry_run": config.dry_run,
        },
        "metadata": config.extra_metadata,
    }
    return summary


__all__ = [
    "ConditionalSamplingConfig",
    "ConditionalSamplingResult",
    "run_conditional_sampling",
]
